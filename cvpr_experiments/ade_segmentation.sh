# Multi-GPU training
RUN_NAME="ade20k_80k_blipmin40+LS"; GPUS=2; NNODES=${NNODES:-1}; NODE_RANK=${NODE_RANK:-0}; PORT=${PORT:-29500}; MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}; python -m torch.distributed.launch --node_rank=$NODE_RANK --master_addr=$MASTER_ADDR --nproc_per_node=$GPUS --master_port=$PORT train_tadp_mm.py TADP/mm_configs/seg_ade20k_full.py --launcher pytorch --text_conditioning blip --blip_caption_path captions/ade20k_captions_min=40_max=77.json --use_scaled_encode
